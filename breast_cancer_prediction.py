# -*- coding: utf-8 -*-
"""Breast_Cancer_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TIxo2FqDyeuyJPwFluiUi7q6d31eIapc
"""

#import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#load the data
df = pd.read_csv('Breast_Cancer.csv')
print(df.shape)
print(df.dtypes)
df.head(5)

#Checking of NULL values in each column
df.isna().sum()

#Removing unnecessery cells
df.drop(['id'], axis=1, inplace=True)

#Find duplicate records
print(f'Number of duplicated records: {df.duplicated().sum()}')

#Find the unique value of 'diagnosis'
print(f"\nUnique values of diagnosis: {df['diagnosis'].unique()}")

#Visualize diagnosis
plt.figure(figsize=(5,5))
sns.countplot(x=df['diagnosis'], label='Count', palette=['#FF0080',"#0E86D4"])


#Number of patients having Malignant(M) or Benign(B) cells
malignant = df[df['diagnosis'] == 'M']['diagnosis'].count()
benign = df[df['diagnosis'] == 'B']['diagnosis'].count()
print(f'Malignant: {malignant}')
print(f'Benign: {benign}')

# counter plot of feature radius_mean
plt.figure(figsize=(20, 8))
sns.countplot(df['radius_mean'])

#Generate Statistics
df.describe()

#Plotting the data
g = sns.PairGrid(df.iloc[:, 0:33], hue="diagnosis", palette="Set2")
g = g.map_diag(plt.hist, edgecolor="w")
g = g.map_offdiag(plt.scatter, edgecolor="w", s=40)
plt.show()

#Visualizing the correlation of the columns
plt.figure(figsize=(25,25))
sns.heatmap(df.iloc[:, 0:31].corr(), cmap='RdPu', annot=True, fmt='.0%')

#Encode the catagorical data values
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})

# heatmap of DataFrame
plt.figure(figsize=(16, 9))
sns.heatmap(df)

#Split the data set into independent(X) and dependent(Y) data sets
X = df.iloc[:, 1:31].values
Y = df.iloc[:, 0].values

#Split the data set into 75% training and 25% testing data sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)

#Checking the split of each shape
print(f'\nSize of training data: {X_train.shape}')
print(f'Size of training data: {X_test.shape}')

#Scale the data (feature scaling)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

#Logistic Regression
from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state = 0)
log.fit(X_train, Y_train)
print('\nlogistic Regression Test Accuracy:',accuracy_score(Y_test, log.predict(X_test)))
print(classification_report(Y_test, log.predict(X_test)))

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
tree.fit(X_train, Y_train)
print('Decision Tree Test Accuracy:',accuracy_score(Y_test, tree.predict(X_test)))
print(classification_report(Y_test, tree.predict(X_test)))

#Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
forest.fit(X_train, Y_train)
print('Random Forest Test Accuracy:',accuracy_score(Y_test, forest.predict(X_test)))
print(classification_report(Y_test, forest.predict(X_test)))

# XGBoost Classifier
from xgboost import XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_train, Y_train)
print('XGBoost Classifier Test Accuracy:', accuracy_score(Y_test, xgb_classifier.predict(X_test)))
print(classification_report(Y_test, xgb_classifier.predict(X_test)))

#Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, forest.predict(X_test))
print('Confusion Matrix: \n', cm)

#Deploying the model
import pickle
pickle.dump(forest, open('Breast_Cancer_Detector.pkl', 'wb'))

#Accuracy
print('Accuracy of Random Forest Classifier Model :',accuracy_score(Y_test, forest.predict(X_test)))